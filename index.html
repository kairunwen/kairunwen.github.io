<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Deepak Pathak and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(123, 77, 77, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  .disabled-link {
    color: #999999;
    cursor: default;
    text-decoration: none;
  }
  </style>
  <link rel="shortcut icon" href="images/apple-touch-ri-logo-white-120x120.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Kairun Wen</title>
  <meta name="Kairun Wen's Homepage" http-equiv="Content-Type" content="Kairun Wen's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-XXXXX-Y', 'auto');
    ga('send', 'pageview');
    </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>
 
<body>
<table width="900" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Kairun Wen 「温凯润」</pageheading><br>
  </p>

  <tr>
    <td width="30%" valign="top"><a href="images/profile_fig/Giraffe.png"><img src="images/profile_fig/Giraffe.png" width="100%" style="border-radius:15px"></a>
    <p align=center>
    | <a >CV</a> |
    <a href="mailto:wenkairun@gmail.com">Email</a> |
    <a href="https://github.com/kairunwen">Github</a> |     
    <a href="https://www.youtube.com/@kairunwen">Youtube</a> |     
    <br/>
    | <a href="https://scholar.google.com/citations?user=RzRhziMAAAAJ&hl=zh-CN">Google Scholar</a> |
    <a href="https://huggingface.co/kairunwen">HuggingFace</a> |
    <br/>
    | <a href="https://www.linkedin.com/in/kairun-wen-9000000000/">LinkedIn</a> |
    <!-- <a href="https://www.instagram.com/kairunwen/">Instagram</a> | -->
    <a href="https://www.xiaohongshu.com/user/profile/5b003ed14eacab54c3be8a61">Xiaohongshu</a> |
    </p>
    <p align="center" style="margin-top:-8px;">
      <iframe id="twitter-widget-0" 
              scrolling="no" 
              frameborder="0" 
              allowtransparency="true" 
              allowfullscreen="true" 
              class="twitter-follow-button twitter-follow-button-rendered" 
              style="position: static; visibility: visible; width: 156px; height: 20px;" 
              title="Twitter Follow Button" 
              src="https://platform.twitter.com/widgets/follow_button.2f70fb173b9000da126c79afe2098f02.en.html#dnt=false&amp;id=twitter-widget-0&amp;lang=en&amp;screen_name=kairunwen&amp;show_count=false&amp;show_screen_name=true&amp;size=m" 
              data-screen-name="kairunwen">
      </iframe>
      <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
  </p>
    </td>
    <td width="70%" valign="top" align="justify">
      <p>I am a second-year master student in the School of Informatics at <a href="https://www.xmu.edu.cn/">Xiamen University</a>, advised by Prof. <a href="https://scholar.google.com/citations?user=k5hVBfMAAAAJ&hl=zh-CN">Xinghao Ding</a> in the SmartDSP group. I am also collaborating with Dr. <a href="https://zhiwenfan.github.io/">Zhiwen Fan</a> and Prof. <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang">Atlas Wang</a> from the <a href="https://vita-group.github.io/">VITA</a> group at the University of Texas at Austin.
        <br>
      </p>
      
      <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; My current research primarily covers the following topics:
        <ul>
          <li><strong style="color: #27AE60;">3D Reconstruction</strong>: Few-Shot and Ultra-Efficient 3D Learning</li>
          <li><strong style="color: #e18a3b;">3D Perception</strong>: Semantic Understanding </li>
          <li><strong style="color: #2092E4;">Computational Photography</strong>: Image Restoration, Low-Light Enhancement</li>
        </ul>
      </p>
     
    </td>
  </tr>
</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Recent News</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
      <ul> 
        <li>[09/2024] Our NeurIPS'24 (<a href="https://lightgaussian.github.io/">LightGaussian</a>) is selected as <strong>spotlight</strong> presentation!</li>

      </ul>
    </td>
  </tr>
  
</table>


<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading> &nbsp;&nbsp; ( * denotes equal contribution )</td></tr>
</table>
 
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://instantsplat.github.io/">
    <video playsinline autoplay loop muted src="images/instantsplat/InstantSplat-preview.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://instantsplat.github.io/" id="InstantSplat">
      <heading>InstantSplat: Sparse-view SfM-free Gaussian Splatting in Seconds</heading></a><br>
      <u>Kairun Wen*</u>, Zhiwen Fan*, Wenyan Cong*, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang<br>
      Preprint 2024<br>
      </p>
      <div class="paper" id="instantsplat">
      <a href="https://instantsplat.github.io/">Project</a> |
      <a href="https://arxiv.org/abs/2403.20309">Paper</a> |      
      <a href="javascript:toggleblock('instantsplat_abs')">Abstract</a> |
      <a shape="rect" href="javascript:togglebib('instantsplat')" class="togglebib">Bibtex</a> |
      
      <a href="https://www.youtube.com/watch?v=_9aQHLHHoEM">Video</a> |
      <a href="https://huggingface.co/spaces/kairunwen/InstantSplat">HF Demo</a> |
      <a href="https://github.com/NVlabs/InstantSplat">Code [1300+⭐]</a> 

      

      <p align="justify"> <i id="instantsplat_abs">
        &nbsp;&nbsp;&nbsp;&nbsp;While neural 3D reconstruction has advanced substantially, it typically requires densely captured multi-view data with carefully initialized poses (e.g., using COLMAP). However, this requirement limits its broader applicability, as Structure-from-Motion (SfM) is often unreliable in sparse-view scenarios where feature matches are limited, resulting in cumulative errors.<br>
        
        &nbsp;&nbsp;&nbsp;&nbsp;In this paper, we introduce <strong>InstantSplat</strong>, a novel and lightning-fast neural reconstruction system that builds accurate 3D representations from as few as 2-3 images. InstantSplat adopts a self-supervised framework that bridges the gap between 2D images and 3D representations using Gaussian Bundle Adjustment (GauBA) and can be optimized in an end-to-end manner. InstantSplat integrates dense stereo priors and co-visibility relationships between frames to initialize pixel-aligned geometry by progressively expanding the scene avoiding redundancy. Gaussian Bundle Adjustment is used to adapt both the scene representation and camera parameters quickly by minimizing gradient-based photometric error. Overall, InstantSplat achieves large-scale 3D reconstruction in mere seconds by reducing the required number of input views, and is compatible with multiple 3D representations (3D-GS, Mip-Splatting). It achieves an acceleration of over 20 times in reconstruction, improves visual quality (SSIM) from 0.3755 to 0.7624 than COLMAP with 3D-GS.</i></p>

<pre xml:space="preserve">
  @misc{fan2024instantsplat,
    title={InstantSplat: Sparse-view Gaussian Splatting in Seconds},
    author={Zhiwen Fan and Kairun Wen and Wenyan Cong and Kevin Wang and Jian Zhang and Xinghao Ding and Danfei Xu and Boris Ivanovic and Marco Pavone and Georgios Pavlakos and Zhangyang Wang and Yue Wang},
    year={2024},
    eprint={2403.20309},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
  }
</pre>
      </div>
    </td>
  </tr>


  <tr>
    <td width="40%" valign="top" align="center"><a href="https://cvpr2025-jarvisir.github.io/">
    <video playsinline autoplay loop muted src="images/javisir/jarvisIR-preview.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://cvpr2025-jarvisir.github.io/" id="JarvisIR">
      <heading>JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration</heading></a><br>
      Yunlong Lin*, Zixu Lin*, Haoyu Chen, Panwang Pan, Chenxin Li, Sixiang Chen, <u>Kairun Wen</u>, Yeying Jin, Wenbo Li, Xinghao Ding <br>
      CVPR 2025 <br>
      </p>

      <div class="paper" id="javisir">
      <a href="https://cvpr2025-jarvisir.github.io/">Project</a> |
      <a href="https://lyl1015.github.io/papers/CVPR2025_JarvisIR.pdf">Paper</a> |
      <a href="javascript:toggleblock('javisir_abs')">Abstract</a> |
      <a shape="rect" href="javascript:togglebib('javisir')" class="togglebib">Bibtex</a> | 
      <a href="https://github.com/LYL1015/JarvisIR">Code</a>
      

      <p align="justify"> <i id="javisir_abs">
        &nbsp;&nbsp;&nbsp;&nbsp;Vision-centric perception systems struggle with unpredictable and coupled weather degradations in the wild. Current solutions are often limited, as they either depend on specific degradation priors or suffer from significant domain gaps. To enable robust and operation in real-world conditions, we propose <strong><span style="color: rgb(32, 146, 228);">JarvisIR</span></strong>, a VLM-powered agent that leverages the VLM as a controller to manage multiple expert restoration models. To further enhance system robustness, reduce hallucinations, and improve generalizability in real-world adverse weather, JarvisIR employs a novel two-stage framework consisting of supervised fine-tuning and human feedback alignment. Specifically, to address the lack of paired data in real-world scenarios, the human feedback alignment enables the VLM to be fine-tuned effectively on large-scale real-world data in an unsupervised manner. To support the training and evaluation of JarvisIR, we introduce CleanBench, a comprehensive dataset consisting of high-quality and large-scale instruction-responses pairs, including 150K synthetic entries and 80K real entries. Extensive experiments demonstrate that JarvisIR exhibits superior decision-making and restoration capabilities. Compared with existing methods, it achieves a 50% improvement in the average of all perception metrics on CleanBench-Real.
      </i></p>

<pre xml:space="preserve">
  @inproceedings{jarvisir2025,
    title={JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration},
    author={Lin, Yunlong and Lin, Zixu and Chen, Haoyu and Pan, Panwang and Li, Chenxin and Chen, Sixiang and Kairun, Wen and Jin, Yeying and Li, Wenbo and Ding, Xinghao},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2025}
  }
</pre>
      </div>
    </td>
  </tr>








  <tr>
    <td width="40%" valign="top" align="center"><a href="https://github.com/dplut/dplut.github.io">
    <video playsinline autoplay loop muted src="images/dplut/DPLUT-preview.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://github.com/dplut/dplut.github.io" id="DPLUT">
      <heading>Unsupervised Low-light Image Enhancement with Lookup Tables and Diffusion Priors</heading></a><br>
      Yunlong Lin*, Zhenqi Fu*, <u>Kairun Wen</u>, Tian Ye, Sixiang Chen, Ge Meng, Yingying Wang, Yue Huang, Xiaotong Tu, Xinghao Ding <br>
      AAAI 2024 <br>
      </p>

      <div class="paper" id="dplut">
      <a href="https://github.com/dplut/dplut.github.io">Project</a> |
      <a href="https://arxiv.org/abs/2409.18899">Paper</a> |
      <a href="javascript:toggleblock('dplut_abs')">Abstract</a> |
      <a shape="rect" href="javascript:togglebib('dplut')" class="togglebib">Bibtex</a> | 
      <a href="https://github.com/LYL1015/DPLUT">Code</a>


      <p align="justify"> <i id="dplut_abs">
        &nbsp;&nbsp;&nbsp;&nbsp;Low-light image enhancement (LIE) aims at precisely and efficiently recovering an image degraded in poor illumination environments. Recent advanced LIE techniques are using deep neural networks, which require lots of low-normal light image pairs, network parameters, and computational resources. As a result, their practicality is limited. In this work, we devise a novel unsupervised LIE framework based on <strong><span style="color: rgb(32, 146, 228);">D</span>iffusion <span style="color: rgb(32, 146, 228);">P</span>riors and <span style="color: rgb(32, 146, 228);">L</span>ook<span style="color: rgb(32, 146, 228);">U</span>p <span style="color: rgb(32, 146, 228);">T</span>ables (DPLUT)</strong> to achieve efficient low-light image recovery. The proposed approach comprises two critical components: a light adjustment lookup table (LLUT) and a noise suppression lookup table (NLUT). LLUT is optimized with a set of unsupervised losses. It aims at predicting pixelwise curve parameters for the dynamic range adjustment of a specific image. NLUT is designed to remove the amplified noise after the light brightens. As diffusion models are sensitive to noise, diffusion priors are introduced to achieve high-performance noise suppression. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods in terms of visual quality and efficiency.
      </i></p>

<pre xml:space="preserve">
  @misc{lin2024unsupervisedlowlightimageenhancement,
    title={Unsupervised Low-light Image Enhancement with Lookup Tables and Diffusion Priors}, 
    author={Yunlong Lin and Zhenqi Fu and Kairun Wen and Tian Ye and Sixiang Chen and Ge Meng and Yingying Wang and Yue Huang and Xiaotong Tu and Xinghao Ding},
    year={2024},
    eprint={2409.18899},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2409.18899}, 
  }
</pre>
      </div>
    </td>
  </tr>







  <tr>
    <td width="40%" valign="top" align="center"><a href="https://lightgaussian.github.io/">
    <video playsinline autoplay loop muted src="images/lightgaussian/LightGaussian-preview.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://lightgaussian.github.io/" id="LightGaussian">
      <heading>LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</heading></a><br>
      Zhiwen Fan*, Kevin Wang*, <u>Kairun Wen</u>, Zehao Zhu, Dejia Xu, Zhangyang Wang <br>
      NeurIPS 2024 <b style="color:rgb(255, 100, 100);">(Spotlight)</b><br>
      </p> 
      <div class="paper" id="lightgaussian">
      <a href="https://lightgaussian.github.io/">Project</a> |
      <a href="https://arxiv.org/abs/2311.17245">Paper</a> |
      <a href="javascript:toggleblock('lightgaussian_abs')">Abstract</a> |
      <a shape="rect" href="javascript:togglebib('lightgaussian')" class="togglebib">Bibtex</a> |
      <a href="https://www.youtube.com/watch?v=470hul75bSM&feature=youtu.be">Video</a> |
      <a href="https://github.com/VITA-Group/LightGaussian">Code [600+⭐]</a> 

      <p align="justify"> <i id="lightgaussian_abs">
        &nbsp;&nbsp;&nbsp;&nbsp;Recent advancements in real-time neural rendering using point-based techniques have paved the way for the widespread adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting come with a substantial storage overhead caused by growing the Structure-from-Motion (SfM) points to millions, often demanding gigabyte-level disk space for a single unbounded scene, posing significant scalability challenges and hindering the splatting efficiency.<br>

        &nbsp;&nbsp;&nbsp;&nbsp;To address this challenge, we introduce <strong>LightGaussian</strong>, a novel method designed to transform 3D Gaussians into a more efficient and compact format. Drawing inspiration from the concept of Network Pruning, LightGaussian identifies Gaussians that are insignificant in contributing to the scene reconstruction and adopts a pruning and recovery process, effectively reducing redundancy in Gaussian counts while preserving visual effects. 
        Additionally, LightGaussian employs knowledge distillation and pseudo-view augmentation to transfer spherical harmonics coefficients to a lower degree, allowing knowledge convert to more compact representations.
        LightGaussian also proposes a Gaussian Vector Quantization based on Gaussian global significance, to quantize all redundant attributes, resulting in lower bitwidth representations with minimal accuracy losses.<br>

        &nbsp;&nbsp;&nbsp;&nbsp;In summary, LightGaussian achieves an average <strong>compression rate exceeding 15x</strong> while boosting the <strong>FPS from 144 to 237</strong> on the representative 3D-GS framework, thereby supporting an efficient representation of complex scenes on Mip-NeRF 360 and Tank \& Temple datasets. The proposed Gaussian pruning approach can also be adapted to other representations (e.g., Scaffold-GS), demonstrating its generalization capability.</i></p>

<pre xml:space="preserve">
  @misc{fan2023lightgaussian, 
    title={LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS}, 
    author={Zhiwen Fan and Kevin Wang and Kairun Wen and Zehao Zhu and Dejia Xu and Zhangyang Wang}, 
    year={2023},
    eprint={2311.17245},
    archivePrefix={arXiv},
    primaryClass={cs.CV} 
  }
</pre>
      </div>
    </td>
  </tr>

  

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://largespatialmodel.github.io/">
    <video playsinline autoplay loop muted src="images/largespatialmodel/LargeSpatialModel-preview.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://largespatialmodel.github.io/" id="LSM">
      <heading>Large Spatial Model: End-to-end Unposed Images to Semantic 3D</heading></a><br>
      Zhiwen Fan*,  Jian Zhang*,  Wenyan Cong,  Peihao Wang,  Renjie Li,  <u>Kairun Wen</u>,  Shijie Zhou,  Achuta Kadambi,  Zhangyang Wang,  Danfei Xu,  Boris Ivanovic,  Marco Pavone,  Yue Wang<br>
      NeurIPS 2024<br>
      </p>

      <div class="paper" id="lsm">
      <a href="https://largespatialmodel.github.io/">Project</a> |
      <a href="https://arxiv.org/abs/2410.18956">Paper</a> |
      <a href="javascript:toggleblock('lsm_abs')">Abstract</a> |
      <a shape="rect" href="javascript:togglebib('lsm')" class="togglebib">Bibtex</a> |       
      <a href="https://github.com/NVlabs/LSM">Code</a> |
      <a href="https://huggingface.co/spaces/kairunwen/LSM">HF Demo</a> 

      <p align="justify"> <i id="lsm_abs">
        &nbsp;&nbsp;&nbsp;&nbsp;Reconstructing and understanding 3D structures from a limited number of images is a classical problem in computer vision. Traditional approaches typically decompose this task into multiple subtasks, involving several stages of complex mappings between different data representations. For example, dense reconstruction using Structure-from-Motion (SfM) requires transforming images into key points, optimizing camera parameters, and estimating structures. Following this, accurate sparse reconstructions are necessary for further dense modeling, which is then input into task-specific neural networks. This multi-stage paradigm leads to significant processing times and engineering complexity.<br>

        &nbsp;&nbsp;&nbsp;&nbsp;In this work, we introduce the <strong><span style="color: rgb(32, 146, 228);">L</span>arge <span style="color: rgb(32, 146, 228);">S</span>patial <span style="color: rgb(32, 146, 228);">M</span>odel (LSM)</strong>, which directly processes unposed RGB images into semantic radiance fields. LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward pass and can synthesize versatile label maps by interacting through language at novel views. Built on a general Transformer-based framework, LSM integrates global geometry via pixel-aligned point maps. To improve spatial attribute regression, we adopt local context aggregation with multi-scale fusion, enhancing the accuracy of fine local details. To address the scarcity of labeled 3D semantic data and enable natural language-driven scene manipulation, we incorporate a pre-trained 2D language-based segmentation model into a 3D-consistent semantic feature field. An efficient decoder parameterizes a set of semantic anisotropic Gaussians, allowing supervised end-to-end learning. Comprehensive experiments on various tasks demonstrate that LSM unifies multiple 3D vision tasks directly from unposed images, achieving real-time semantic 3D reconstruction for the first time.</i></p>

<pre xml:space="preserve">
  @misc{fan2024largespatialmodelendtoend,
    title={Large Spatial Model: End-to-end Unposed Images to Semantic 3D}, 
    author={Zhiwen Fan and Jian Zhang and Wenyan Cong and Peihao Wang and Renjie Li and Kairun Wen and Shijie Zhou and Achuta Kadambi and Zhangyang Wang and Danfei Xu and Boris Ivanovic and Marco Pavone and Yue Wang},
    year={2024},
    eprint={2410.18956},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2410.18956}, 
  }
</pre>
      </div>
    </td>
  </tr>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Honors & Awards</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      National Scholarship <strong>(Top 0.2% Nationwide)</strong>, 2022 
      <br>
      National Scholarship <strong>(Top 0.2% Nationwide)</strong>, 2020
      </p>
    </td>
  </tr>
</table>



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Reviewer Services</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      International Conference on Machine Learning <b>(ICML)</b>, 2025
      <br>
      International Conference on Learning Representations <b>(ICLR)</b>, 2025
      <br>
      Conference on Neural Information Processing Systems <b>(NeurIPS)</b>, 2025, 2024
      </p>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody>
      <tr>
          <td style="padding:0px">
              <br><br>
              <div>
                <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=300&t=tt&d=XA3g-twJ0DTL724FmmOYYY49GazZpsWPVMvjlU6nZjc&co=ffffff&ct=808080&cmo=3acc3a&cmn=ff5353'></script>
              </div>
          </td>
      </tr>
  </tbody>
</table>








<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right">
    Website template from <a href="http://www.cs.berkeley.edu/~barron/">here</a> and <a href="https://tairanhe.com/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('material_review_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('ieee_iot_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('acm_turc_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('aog_mcts_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('pragmatics_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('collab_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('rma_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('energyloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('navloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('lsm_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('lightgaussian_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('instantsplat_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('dplut_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('javisir_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('agile-but-safe_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('safedpa_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('acs_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('saferl_survey_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('patchail_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('sisos_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('uaissa_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('autocost_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('a2ls_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('issa_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('ebil_abs');
</script>
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('maniploco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('parkour_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('mobile_aloha_abs');
</script>
</body>

</html>