<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Deepak Pathak and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(123, 77, 77, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  .disabled-link {
    color: #999999;
    cursor: default;
    text-decoration: none;
  }
  </style>
  <!-- <link rel="shortcut icon" href="images/apple-touch-ri-logo-white-120x120.png"> -->
  <link rel="icon" type="image/x-icon" href="images/profile_fig/Giraffe.ico">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Kairun Wen</title>
  <meta name="Kairun Wen's Homepage" http-equiv="Content-Type" content="Kairun Wen's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-XXXXX-Y', 'auto');
    ga('send', 'pageview');
    </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>
 
<body>
<table width="900" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Kairun Wen „ÄåÊ∏©ÂáØÊ∂¶„Äç</pageheading><br>
  </p>

  <tr>
    <td width="30%" valign="top"><a href="images/profile_fig/Giraffe.png"><img src="images/profile_fig/Giraffe.png" width="100%" style="border-radius:15px"></a>
    <p align=center>
    | <a >CV</a> |
    <a href="mailto:wenkairun@gmail.com">Email</a> |
    <a href="https://github.com/kairunwen">Github</a> |
    <br/>
    | <a href="https://scholar.google.com/citations?user=RzRhziMAAAAJ&hl=zh-CN">Google Scholar</a> |
    <a href="https://huggingface.co/kairunwen">HuggingFace</a> |
    <br/>
    | <a href="https://www.linkedin.com/in/kairun-wen-9000000000/">LinkedIn</a> |
    <!-- <a href="https://www.instagram.com/kairunwen/">Instagram</a> | -->
    <a href="https://www.xiaohongshu.com/user/profile/5b003ed14eacab54c3be8a61">RedNote</a> |    
    <a href="https://www.youtube.com/@kairunwen">Youtube</a> |  
    </p>
    

    <p align="center" style="margin-top:-8px;">
      <iframe id="twitter-widget-0" 
              scrolling="no" 
              frameborder="0" 
              allowtransparency="true" 
              allowfullscreen="true" 
              class="twitter-follow-button twitter-follow-button-rendered" 
              style="position: static; visibility: visible; width: 156px; height: 20px;" 
              title="Twitter Follow Button" 
              src="https://platform.twitter.com/widgets/follow_button.2f70fb173b9000da126c79afe2098f02.en.html#dnt=false&amp;id=twitter-widget-0&amp;lang=en&amp;screen_name=kairunwen&amp;show_count=false&amp;show_screen_name=true&amp;size=m" 
              data-screen-name="kairunwen">
      </iframe>
      <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
  </p>
    </td>
    <td width="70%" valign="top" align="justify">
      <p>    
        I am an incoming Ph.D. student at <a href="https://mmlab.ie.cuhk.edu.hk/">MMLab</a>, <a href="https://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>, advised by Prof. <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>. Currently, I am pursuing my master‚Äôs degree in the School of Informatics at <a href="https://www.xmu.edu.cn/">Xiamen University</a>, advised by Prof. <a href="https://scholar.google.com/citations?user=k5hVBfMAAAAJ&hl=zh-CN">Xinghao Ding</a>. I have research experience with leading research groups across academia and industry, including <a href="https://www.vita-group.space/">VITA-Group@UT Austin</a> and  <a href="https://www.vita-group.space/">InternRobotics@Shanghai AI Lab</a>. My long-term research goal is to develop an efficient and scalable Embodied Agent that can continuously perceive, understand and interact with the physical world through Multi-modal Spatial Intelligence.
        <br>
      </p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; My current research primarily covers the following topics:
        <ul>
          <li><strong style="color: #e18a3b;">3D/4D World Modeling and Scene Understanding</strong>: Physically grounded modeling of dynamic scenes and spatio-temporal reasoning.</li>
          <li><strong style="color: #27AE60;">Efficient 3D Representation Learning</strong>: Few-shot, data-efficient, and compute-efficient 3D learning for large-scale environments.</li>
          <li><strong style="color: #9A2036;">Embodied AI and Interactive Agents</strong>: Planning, decision-making, and reinforcement learning for agents acting in the physical world.</li>
        </ul>
      </p>
      <!-- <p style="color: #e60012;">I am currently looking for <strong>26 fall PhD positions</strong> and actively seeking collaborations! If you are interested in working together or have potential PhD opportunities, please feel free to reach out to me.</p> -->
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>WeChat</strong>: kairun_wen &nbsp;&nbsp; <strong>Email</strong>: wenkairun@gmail.com        
      </p>
     
    </td>
  </tr>
</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Recent News</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
      <ul>         
        <li>[09/2025] Two papers were accepted to NeurIPS'25 üéâ!</li>
        <li>[09/2024] Our NeurIPS'24 (<a href="https://lightgaussian.github.io/">LightGaussian</a>) is selected as <strong>spotlight</strong> presentation!</li>
      </ul>
    </td>
  </tr>
  
</table>


<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading> &nbsp;&nbsp; ( * denotes equal contribution, ‚Ä† denotes project lead )</td></tr>
</table>
 
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">




  <!-- <tr>
    <td width="40%" valign="top" align="center"><a href="https://instantsplat.github.io/">
    <video playsinline autoplay loop muted src="images/sam3r/SAM3R-preview.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://instantsplat.github.io/" id="SAM3R">
      <heading>SAM3R: Language-Grounded Segment Anything Model for 3D Reconstruction in Real-time</heading></a><br>
      <u>Kairun Wen</u><br>
      Ongoing<br>
      </p>
      <div class="paper" id="sam3r">           
      <a href="javascript:toggleblock('sam3r_abs')">Abstract</a> 

      

      <p align="justify"> <i id="sam3r_abs">
&nbsp;&nbsp;&nbsp;&nbsp;Embodied agents are expected to fully understand the 3D physical  world simultaneously with exploration. This necessitates agents equipped with robust spatial reasoning and semantic understanding, requiring pretraining on massive data that goes beyond mere pixels.
However, due to the scarcity of the high-quality 3D data and 3D-language paired data, directly training such a model is almost infeasible.
        <br>
        &nbsp;&nbsp;&nbsp;&nbsp;To this end, we propose a data engine that leverages foundation models to automatically generate and refine pseudo-labels for model training with minimal manual supervision.
Our model, called  <strong>SAM3R</strong> (Segment Anything Model for 3D Reconstruction), an online, real-time, RGB-only and Language-grounded 4D perception method that can segment and retrieve any 3D instance given textual queries interactively.
    Specifically, we first leverage SAM and CLIP to synergistically generate 2D semantic priors, then align these priors with the scene geometry in 3D space using calibrated multi-modal data, and ultimately obtain the 3D-aware pseudo-labels. By training on these labels, we distill the semantically-rich knowledge from 2D foundation models into our SAM3R model.
We evaluate our method on various 3D perception tasks and demonstrate competitive or state-of-the-art performance in each. We showcase new avenues for processing arbitrary web-scale monocular video data in an online fashion for unconstrained embodied scenarios where depth and semantics are absent.</i></p>

<pre xml:space="preserve">
  @misc{fan2024instantsplat,
    title={InstantSplat: Sparse-view Gaussian Splatting in Seconds},
    author={Zhiwen Fan and Kairun Wen and Wenyan Cong and Kevin Wang and Jian Zhang and Xinghao Ding and Danfei Xu and Boris Ivanovic and Marco Pavone and Georgios Pavlakos and Zhangyang Wang and Yue Wang},
    year={2024},
    eprint={2403.20309},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
  }
</pre>
      </div>
    </td>
  </tr> -->




  <tr>
    <td width="40%" valign="top" align="center"><a href="https://dynamic-verse.github.io/">
    <video playsinline autoplay loop muted src="images/dynamicverse/DynamicVerse-preview.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://dynamic-verse.github.io/" id="DynamicVerse">
      <heading>DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling</heading></a><br>
      <u>Kairun Wen*‚Ä†</u>, Yuzhi Huang*, Runyu Chen, Hui Zheng, Yunlong Lin, Panwang Pan, Chenxin Li, Wenyan Cong, Jian Zhang, Junbin Lu, Chenguo Lin, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Yue Huang, Xinghao Ding, Rakesh Ranjan, Zhiwen Fan<br>
      NeurIPS 2025<br>
      </p>
      <div class="paper" id="dynamicverse">
      <a href="https://dynamic-verse.github.io/">Project</a> |
      <a href="https://arxiv.org/abs/2512.00300">Paper</a> |      
      <a href="javascript:toggleblock('dynamicverse_abs')">Abstract</a> |
      <a shape="rect" href="javascript:togglebib('dynamicverse')" class="togglebib">Bibtex</a> |      
      <a href="https://www.youtube.com/watch?v=0h7XysIpG8Y">Video</a> |
      <a href="https://huggingface.co/spaces/kairunwen/InstantSplat">HF Data</a> |
      <a href="https://github.com/Dynamics-X/DynamicVerse">Code</a> 

      

      <p align="justify"> <i id="dynamicverse_abs">
        &nbsp;&nbsp;&nbsp;&nbsp;Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet.<br>

        &nbsp;&nbsp;&nbsp;&nbsp;To bridge these gaps, we introduce <strong>DynamicVerse</strong>, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consisting of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.</i></p>

<pre xml:space="preserve">
  @misc{wen2025dynamicverse,
        title={DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling}, 
        author={Kairun Wen and Yuzhi Huang and Runyu Chen and Hui Zheng and Yunlong Lin and Panwang Pan and Chenxin Li and Wenyan Cong and Jian Zhang and Junbin Lu and Chenguo Lin and Dilin Wang and Zhicheng Yan and Hongyu Xu and Justin Theiss and Yue Huang and Xinghao Ding and Rakesh Ranjan and Zhiwen Fan},
        year={2025},
        eprint={2512.03000},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2512.03000}, 
    }
</pre>
      </div>
    </td>
  </tr>










  <tr>
    <td width="40%" valign="top" align="center"><a href="https://instantsplat.github.io/">
    <video playsinline autoplay loop muted src="images/instantsplat/InstantSplat-preview.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://instantsplat.github.io/" id="InstantSplat">
      <heading>InstantSplat: Sparse-view SfM-free Gaussian Splatting in Seconds</heading></a><br>
      Zhiwen Fan*, <u>Kairun Wen*</u>, Wenyan Cong*, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, Zhangyang Wang, Yue Wang<br>
      Preprint<br>
      </p>
      <div class="paper" id="instantsplat">
      <a href="https://instantsplat.github.io/">Project</a> |
      <a href="https://arxiv.org/abs/2403.20309">Paper</a> |      
      <a href="javascript:toggleblock('instantsplat_abs')">Abstract</a> |
      <a shape="rect" href="javascript:togglebib('instantsplat')" class="togglebib">Bibtex</a> |
      
      <a href="https://www.youtube.com/watch?v=_9aQHLHHoEM">Video</a> |
      <a href="https://huggingface.co/spaces/kairunwen/InstantSplat">HF Demo</a> |
      <a href="https://github.com/NVlabs/InstantSplat">Code [1600+‚≠ê]</a> 

      

      <p align="justify"> <i id="instantsplat_abs">
        &nbsp;&nbsp;&nbsp;&nbsp;While neural 3D reconstruction has advanced substantially, it typically requires densely captured multi-view data with carefully initialized poses (e.g., using COLMAP). However, this requirement limits its broader applicability, as Structure-from-Motion (SfM) is often unreliable in sparse-view scenarios where feature matches are limited, resulting in cumulative errors.<br>
        
        &nbsp;&nbsp;&nbsp;&nbsp;In this paper, we introduce <strong>InstantSplat</strong>, a novel and lightning-fast neural reconstruction system that builds accurate 3D representations from as few as 2-3 images. InstantSplat adopts a self-supervised framework that bridges the gap between 2D images and 3D representations using Gaussian Bundle Adjustment (GauBA) and can be optimized in an end-to-end manner. InstantSplat integrates dense stereo priors and co-visibility relationships between frames to initialize pixel-aligned geometry by progressively expanding the scene avoiding redundancy. Gaussian Bundle Adjustment is used to adapt both the scene representation and camera parameters quickly by minimizing gradient-based photometric error. Overall, InstantSplat achieves large-scale 3D reconstruction in mere seconds by reducing the required number of input views, and is compatible with multiple 3D representations (3D-GS, Mip-Splatting). It achieves an acceleration of over 20 times in reconstruction, improves visual quality (SSIM) from 0.3755 to 0.7624 than COLMAP with 3D-GS.</i></p>

<pre xml:space="preserve">
  @misc{fan2024instantsplat,
    title={InstantSplat: Sparse-view Gaussian Splatting in Seconds},
    author={Zhiwen Fan and Kairun Wen and Wenyan Cong and Kevin Wang and Jian Zhang and Xinghao Ding and Danfei Xu and Boris Ivanovic and Marco Pavone and Georgios Pavlakos and Zhangyang Wang and Yue Wang},
    year={2024},
    eprint={2403.20309},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
  }
</pre>
      </div>
    </td>
  </tr>


  <tr>
    <td width="40%" valign="top" align="center"><a href="https://cvpr2025-jarvisir.github.io/">
    <video playsinline autoplay loop muted src="images/javisir/jarvisIR-preview.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://cvpr2025-jarvisir.github.io/" id="JarvisIR">
      <heading>JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration</heading></a><br>
      Yunlong Lin*, Zixu Lin*, Haoyu Chen*, Panwang Pan*, Chenxin Li, Sixiang Chen, <u>Kairun Wen</u>, Yeying Jin, Wenbo Li, Xinghao Ding <br>
      CVPR 2025 <br>
      </p>

      <div class="paper" id="javisir">
      <a href="https://cvpr2025-jarvisir.github.io/">Project</a> |
      <a href="https://lyl1015.github.io/papers/CVPR2025_JarvisIR.pdf">Paper</a> |
      <a href="javascript:toggleblock('javisir_abs')">Abstract</a> |
      <a shape="rect" href="javascript:togglebib('javisir')" class="togglebib">Bibtex</a> | 
      <a href="https://github.com/LYL1015/JarvisIR">Code</a>
      

      <p align="justify"> <i id="javisir_abs">
        &nbsp;&nbsp;&nbsp;&nbsp;Vision-centric perception systems struggle with unpredictable and coupled weather degradations in the wild. Current solutions are often limited, as they either depend on specific degradation priors or suffer from significant domain gaps. To enable robust and operation in real-world conditions, we propose <strong><span style="color: rgb(32, 146, 228);">JarvisIR</span></strong>, a VLM-powered agent that leverages the VLM as a controller to manage multiple expert restoration models. To further enhance system robustness, reduce hallucinations, and improve generalizability in real-world adverse weather, JarvisIR employs a novel two-stage framework consisting of supervised fine-tuning and human feedback alignment. Specifically, to address the lack of paired data in real-world scenarios, the human feedback alignment enables the VLM to be fine-tuned effectively on large-scale real-world data in an unsupervised manner. To support the training and evaluation of JarvisIR, we introduce CleanBench, a comprehensive dataset consisting of high-quality and large-scale instruction-responses pairs, including 150K synthetic entries and 80K real entries. Extensive experiments demonstrate that JarvisIR exhibits superior decision-making and restoration capabilities. Compared with existing methods, it achieves a 50% improvement in the average of all perception metrics on CleanBench-Real.
      </i></p>

<pre xml:space="preserve">
  @inproceedings{jarvisir2025,
    title={JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration},
    author={Lin, Yunlong and Lin, Zixu and Chen, Haoyu and Pan, Panwang and Li, Chenxin and Chen, Sixiang and Kairun, Wen and Jin, Yeying and Li, Wenbo and Ding, Xinghao},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2025}
  }
</pre>
      </div>
    </td>
  </tr>












  <tr>
    <td width="40%" valign="top" align="center"><a href="https://lightgaussian.github.io/">
    <video playsinline autoplay loop muted src="images/lightgaussian/LightGaussian-preview.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://lightgaussian.github.io/" id="LightGaussian">
      <heading>LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS</heading></a><br>
      Zhiwen Fan*, Kevin Wang*, <u>Kairun Wen</u>, Zehao Zhu, Dejia Xu, Zhangyang Wang <br>
      NeurIPS 2024 <b style="color:rgb(255, 100, 100);">(Spotlight)</b><br>
      </p> 
      <div class="paper" id="lightgaussian">
      <a href="https://lightgaussian.github.io/">Project</a> |
      <a href="https://arxiv.org/abs/2311.17245">Paper</a> |
      <a href="javascript:toggleblock('lightgaussian_abs')">Abstract</a> |
      <a shape="rect" href="javascript:togglebib('lightgaussian')" class="togglebib">Bibtex</a> |
      <a href="https://www.youtube.com/watch?v=470hul75bSM&feature=youtu.be">Video</a> |
      <a href="https://github.com/VITA-Group/LightGaussian">Code [700+‚≠ê]</a> 

      <p align="justify"> <i id="lightgaussian_abs">
        &nbsp;&nbsp;&nbsp;&nbsp;Recent advancements in real-time neural rendering using point-based techniques have paved the way for the widespread adoption of 3D representations. However, foundational approaches like 3D Gaussian Splatting come with a substantial storage overhead caused by growing the Structure-from-Motion (SfM) points to millions, often demanding gigabyte-level disk space for a single unbounded scene, posing significant scalability challenges and hindering the splatting efficiency.<br>

        &nbsp;&nbsp;&nbsp;&nbsp;To address this challenge, we introduce <strong>LightGaussian</strong>, a novel method designed to transform 3D Gaussians into a more efficient and compact format. Drawing inspiration from the concept of Network Pruning, LightGaussian identifies Gaussians that are insignificant in contributing to the scene reconstruction and adopts a pruning and recovery process, effectively reducing redundancy in Gaussian counts while preserving visual effects. 
        Additionally, LightGaussian employs knowledge distillation and pseudo-view augmentation to transfer spherical harmonics coefficients to a lower degree, allowing knowledge convert to more compact representations.
        LightGaussian also proposes a Gaussian Vector Quantization based on Gaussian global significance, to quantize all redundant attributes, resulting in lower bitwidth representations with minimal accuracy losses.<br>

        &nbsp;&nbsp;&nbsp;&nbsp;In summary, LightGaussian achieves an average <strong>compression rate exceeding 15x</strong> while boosting the <strong>FPS from 144 to 237</strong> on the representative 3D-GS framework, thereby supporting an efficient representation of complex scenes on Mip-NeRF 360 and Tank \& Temple datasets. The proposed Gaussian pruning approach can also be adapted to other representations (e.g., Scaffold-GS), demonstrating its generalization capability.</i></p>

<pre xml:space="preserve">
  @misc{fan2023lightgaussian, 
    title={LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS}, 
    author={Zhiwen Fan and Kevin Wang and Kairun Wen and Zehao Zhu and Dejia Xu and Zhangyang Wang}, 
    year={2023},
    eprint={2311.17245},
    archivePrefix={arXiv},
    primaryClass={cs.CV} 
  }
</pre>
      </div>
    </td>
  </tr>

  

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://largespatialmodel.github.io/">
    <video playsinline autoplay loop muted src="images/largespatialmodel/LargeSpatialModel-preview.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://largespatialmodel.github.io/" id="LSM">
      <heading>Large Spatial Model: End-to-end Unposed Images to Semantic 3D</heading></a><br>
      Zhiwen Fan*,  Jian Zhang*,  Wenyan Cong,  Peihao Wang,  Renjie Li,  <u>Kairun Wen</u>,  Shijie Zhou,  Achuta Kadambi,  Zhangyang Wang,  Danfei Xu,  Boris Ivanovic,  Marco Pavone,  Yue Wang<br>
      NeurIPS 2024<br>
      </p>

      <div class="paper" id="lsm">
      <a href="https://largespatialmodel.github.io/">Project</a> |
      <a href="https://arxiv.org/abs/2410.18956">Paper</a> |
      <a href="javascript:toggleblock('lsm_abs')">Abstract</a> |
      <a shape="rect" href="javascript:togglebib('lsm')" class="togglebib">Bibtex</a> |       
      <a href="https://github.com/NVlabs/LSM">Code</a> |
      <a href="https://huggingface.co/spaces/kairunwen/LSM">HF Demo</a> 

      <p align="justify"> <i id="lsm_abs">
        &nbsp;&nbsp;&nbsp;&nbsp;Reconstructing and understanding 3D structures from a limited number of images is a classical problem in computer vision. Traditional approaches typically decompose this task into multiple subtasks, involving several stages of complex mappings between different data representations. For example, dense reconstruction using Structure-from-Motion (SfM) requires transforming images into key points, optimizing camera parameters, and estimating structures. Following this, accurate sparse reconstructions are necessary for further dense modeling, which is then input into task-specific neural networks. This multi-stage paradigm leads to significant processing times and engineering complexity.<br>

        &nbsp;&nbsp;&nbsp;&nbsp;In this work, we introduce the <strong><span style="color: rgb(32, 146, 228);">L</span>arge <span style="color: rgb(32, 146, 228);">S</span>patial <span style="color: rgb(32, 146, 228);">M</span>odel (LSM)</strong>, which directly processes unposed RGB images into semantic radiance fields. LSM simultaneously estimates geometry, appearance, and semantics in a single feed-forward pass and can synthesize versatile label maps by interacting through language at novel views. Built on a general Transformer-based framework, LSM integrates global geometry via pixel-aligned point maps. To improve spatial attribute regression, we adopt local context aggregation with multi-scale fusion, enhancing the accuracy of fine local details. To address the scarcity of labeled 3D semantic data and enable natural language-driven scene manipulation, we incorporate a pre-trained 2D language-based segmentation model into a 3D-consistent semantic feature field. An efficient decoder parameterizes a set of semantic anisotropic Gaussians, allowing supervised end-to-end learning. Comprehensive experiments on various tasks demonstrate that LSM unifies multiple 3D vision tasks directly from unposed images, achieving real-time semantic 3D reconstruction for the first time.</i></p>

<pre xml:space="preserve">
  @misc{fan2024largespatialmodelendtoend,
    title={Large Spatial Model: End-to-end Unposed Images to Semantic 3D}, 
    author={Zhiwen Fan and Jian Zhang and Wenyan Cong and Peihao Wang and Renjie Li and Kairun Wen and Shijie Zhou and Achuta Kadambi and Zhangyang Wang and Danfei Xu and Boris Ivanovic and Marco Pavone and Yue Wang},
    year={2024},
    eprint={2410.18956},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2410.18956}, 
  }
</pre>
      </div>
    </td>
  </tr>








  <!-- <tr>
    <td width="40%" valign="top" align="center"><a href="https://github.com/dplut/dplut.github.io">
    <video playsinline autoplay loop muted src="images/dplut/DPLUT-preview.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://github.com/dplut/dplut.github.io" id="DPLUT">
      <heading>Unsupervised Low-light Image Enhancement with Lookup Tables and Diffusion Priors</heading></a><br>
      Yunlong Lin*, Zhenqi Fu*, <u>Kairun Wen</u>, Tian Ye, Sixiang Chen, Ge Meng, Yingying Wang, Yue Huang, Xiaotong Tu, Xinghao Ding <br>
      AAAI 2024 <br>
      </p>

      <div class="paper" id="dplut">
      <a href="https://github.com/dplut/dplut.github.io">Project</a> |
      <a href="https://arxiv.org/abs/2409.18899">Paper</a> |
      <a href="javascript:toggleblock('dplut_abs')">Abstract</a> |
      <a shape="rect" href="javascript:togglebib('dplut')" class="togglebib">Bibtex</a> | 
      <a href="https://github.com/LYL1015/DPLUT">Code</a>


      <p align="justify"> <i id="dplut_abs">
        &nbsp;&nbsp;&nbsp;&nbsp;Low-light image enhancement (LIE) aims at precisely and efficiently recovering an image degraded in poor illumination environments. Recent advanced LIE techniques are using deep neural networks, which require lots of low-normal light image pairs, network parameters, and computational resources. As a result, their practicality is limited. In this work, we devise a novel unsupervised LIE framework based on <strong><span style="color: rgb(32, 146, 228);">D</span>iffusion <span style="color: rgb(32, 146, 228);">P</span>riors and <span style="color: rgb(32, 146, 228);">L</span>ook<span style="color: rgb(32, 146, 228);">U</span>p <span style="color: rgb(32, 146, 228);">T</span>ables (DPLUT)</strong> to achieve efficient low-light image recovery. The proposed approach comprises two critical components: a light adjustment lookup table (LLUT) and a noise suppression lookup table (NLUT). LLUT is optimized with a set of unsupervised losses. It aims at predicting pixelwise curve parameters for the dynamic range adjustment of a specific image. NLUT is designed to remove the amplified noise after the light brightens. As diffusion models are sensitive to noise, diffusion priors are introduced to achieve high-performance noise suppression. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods in terms of visual quality and efficiency.
      </i></p>

<pre xml:space="preserve">
  @misc{lin2024unsupervisedlowlightimageenhancement,
    title={Unsupervised Low-light Image Enhancement with Lookup Tables and Diffusion Priors}, 
    author={Yunlong Lin and Zhenqi Fu and Kairun Wen and Tian Ye and Sixiang Chen and Ge Meng and Yingying Wang and Yue Huang and Xiaotong Tu and Xinghao Ding},
    year={2024},
    eprint={2409.18899},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2409.18899}, 
  }
</pre>
      </div>
    </td>
  </tr> -->



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Honors & Awards</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      National Scholarship <strong>(Top 0.2% Nationwide)</strong>, 2022 
      <br>
      National Scholarship <strong>(Top 0.2% Nationwide)</strong>, 2020
      </p>
    </td>
  </tr>
</table>



<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Reviewer Services</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      International Conference on Machine Learning <b>(ICML)</b>, 2025
      <br>
      International Conference on Learning Representations <b>(ICLR)</b>, 2026, 2025
      <br>
      Conference on Neural Information Processing Systems <b>(NeurIPS)</b>, 2025, 2024
      <br>
      AAAI Conference on Artificial Intelligence <b>(AAAI)</b>, 2026
      </p>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody>
      <tr>
          <td style="padding:0px">
              <br><br>
              <div>
                <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=300&t=tt&d=XA3g-twJ0DTL724FmmOYYY49GazZpsWPVMvjlU6nZjc&co=ffffff&ct=808080&cmo=3acc3a&cmn=ff5353'></script>
              </div>
          </td>
      </tr>
  </tbody>
</table>








<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right">
    Website template from <a href="http://www.cs.berkeley.edu/~barron/">here</a> and <a href="https://tairanhe.com/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('material_review_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('ieee_iot_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('acm_turc_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('aog_mcts_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('pragmatics_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('collab_marl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('rma_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('energyloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('navloco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('lsm_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('lightgaussian_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('instantsplat_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('dynamicverse_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('sam3r_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('dplut_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('javisir_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('agile-but-safe_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('safedpa_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('acs_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('saferl_survey_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('patchail_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('sisos_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('uaissa_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('autocost_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('a2ls_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('issa_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('ebil_abs');
</script>
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('maniploco_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('parkour_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('mobile_aloha_abs');
</script>
</body>

</html>